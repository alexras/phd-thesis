\chapter{Background}
\label{chapter:background}

This section makes the problem domains tackled by TritonSort and Themis more
concrete, and describes the architectural features that both systems share in
common.

\section{Problem Formulation: Sorting}

TritonSort seeks to meet the specifications laid out in the GraySort
benchmark~\cite{terasort}. For this benchmark, the data to be sorted consists of
100 byte records, each of which has a 10-byte key and a 90-byte value. We
target deployments with input datasets that are tens to hundreds of terabytes
in size; the GraySort benchmark's current data size is 100 terabytes.

Input data is stored as a collection of files across the cluster's
disks. TritonSort's goal is to transform this input data set into an ordered
set of output files, also stored across the cluster's disks, such that an
in-order concatenation of these output files is a sorted permutation of the
input data set.

Sorting large datasets places great stress on a cluster's resources.  First,
storing tens to hundreds of terabytes of data demands a large amount of storage
capacity. Given the capacity of modern hard drives, the data must be stored
across several drives and almost certainly across many machines. Second,
performing reads and writes to all these disks simultaneously places load on
both the disks themselves and the I/O controllers connecting them to the
CPU. Third, since the records to be sorted are assumed to be distributed
randomly across input files, almost all of the dataset to be sorted will have
to be sent over the network at some point. Finally, comparing records requires
a non-trivial amount of compute power. This combination of demands makes it
challenging to design an efficient large-scale sorting system that utilizes the
cluster's resources well.

\section{Problem Formulation: MapReduce}

As mentioned in Chapter~\ref{chapter:introduction}, a MapReduce computation is
specified by two functions, \map and \reduce, with \map responsible for
per-record processing and \reduce responsible for aggregation. MapReduce treats
a data set as a collection of \emph{records}, each of which consists of a
\emph{key} and a \emph{value}. Both the key and the value can be arbitrary. A
record with key $k$ and value $v$ will be denoted \kvpair{k}{v}.  Throughout
this dissertation we will refer to records that are produced by the \map
function as \emph{intermediate records} or \emph{mapped records} and records
produced by the \reduce function as \emph{output records} or \emph{reduced
  records}.

A canonical example MapReduce job is the problem of counting the occurrences of
each word in a text corpus. For this problem, the user might write a \map
function that takes a line of text as input and produces the record
\kvpair{word}{1} for each word in the line. The \reduce function would then
receive all records for a given word, add their values together, and produce a
single record \kvpair{word}{n}.

The \map and \reduce functions can produce an arbitrary number of
arbitrarily-sized records. This is in sharp contrast to the GraySort benchmark,
where records can be assumed to be the same size. Additionally, keys can be
arbitrarily distributed throughout the space of possible keys. This makes the
problem of evenly dividing key ranges among nodes difficult, as we will see in
Chapter~\ref{chapter:themis}.

Since each \reduce function is responsible for processing all records with the
same key, the system running a MapReduce job must ensure that all records with
the same key are available on the same node. This property requires that the
system perform a distributed sort of all intermediate records by key before
applying the \reduce function to each key's records. In this way, the problem
of efficiently running MapReduce jobs is a superset of the problem of
efficiently sorting at scale; in fact, a sort job in MapReduce is simply a job
with ``no-op'' \map and \reduce functions that emit any records they receive
unmodified. As we will see later in this dissertation, we applied many of the
lessons learned in designing an efficient large-scale sorting system to the
problem of building an efficient MapReduce platform.
