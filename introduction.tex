\chapter{Introduction}
\label{chapter:introduction}

The quantity of data the world generates and stores is growing at a staggering
rate. Walmart handles more than a million customer transactions per hour,
and the size of its customer database is estimated at 2.5
petabytes~\cite{economist-data-data-everywhere}. Search engines like Google
construct complex indices over the entire public Internet, which is estimated
to consist of at least 14 billion pages~\cite{worldwidewebsize}. Facebook's
users upload more than 300 million photos per
day~\cite{jay-parikh-slideshow}. Scientific instruments like the Australian
Square Kilometer Array, the Large Hadron Collider and the Pan-STARRS array of
telescopes can generate petabytes of data per day~\cite{fourth-paradigm}.

Capturing this data, while a technically challenging feat in and of itself, is
not enough. To be useful, the data must be analyzed, aggregated, filtered, and
transformed. The aforementioned data sets are but a few examples of a new class
of ``big data'' -- data sets that are so large and complex that they become
difficult to process using traditional techniques and technologies.

Some data-intensive problems allow every record to be processed in parallel
without knowing anything about any other records. These problems are known as
``embarrassingly parallel'', and can be scaled out easily.  An example of an
embarrassingly parallel problem is searching a corpus of text for occurrences
of a word; each document in the corpus can be scanned independently and the
results of the scan over each document can be trivially merged together
afterward.

However, a much larger class of problems are not embarrassingly parallel. These
problems require some form of aggregation or combination across records
(analogous to the group-by and join operations in relational database systems)
in addition to per-record processing. One canonical example of this class of
problems is counting the number of times each word occurs in a corpus of
text. The occurrences of each word can be counted in each document
independently, but these counts must be subsequently added together. Performing
this aggregation efficiently is one of the primary challenges facing designers
of systems for processing ``big data''.

\section{The Rise of Partition-Parallel Architectures}

In recent years, a range of large-scale, data-intensive systems have been
developed to tackle jobs like the word count example above.  One of the most
popular frameworks for this form of analysis is MapReduce~\cite{mapreduce}. A
MapReduce computation is specified by two functions. The first function, \map,
takes a record as input and produces zero or more records; it performs the
per-record processing portion of the job. The second function, \reduce, takes
all records with the same key as input and produces zero or more records; it
performs the aggregation portion of the job. Both \map and \reduce functions
are assumed to be deterministic and side-effect free, although this
is sometimes not the case in practice.

MapReduce's strength lies in the simplicity of its programming model. Users of
MapReduce need only write \map and \reduce functions without concerning
themselves with dividing the data among nodes, performing inter-node
communication or recovering from failures. The \map function's signature and its
idempotent nature make it embarrassingly parallel, while the \reduce function's
parallelism can be adjusted from completely serial to extremely parallel based
on the number of distinct keys in the records produced by the \map function.

MapReduce was developed by Google in the early 2000s for tasks like inverted
index generation and PageRank~\cite{pagerank} computation over Google's cache
of the web. Engineers at Yahoo! wrote an open-source version of MapReduce
called Hadoop~\cite{hadoop} in 2005 that has since become extremely popular and
is widely deployed in both academic and industrial settings.

\section{Scale-Out, but not Scale-Up}

While systems like MapReduce scale quite well, they do not utilize their
clusters' resources to nearly the extent that they should. As one example, in
2009 a cluster of 3452 nodes running Hadoop sorted 100 TB of data in 173
minutes~\cite{hadoop-sort-2009}. At a high level, this performance is quite
impressive -- an average of 578 GB of data sorted per minute. However, this
high-level performance masks a great deal of inefficiency. In the
aforementioned record-setting sort, each node in the cluster's average rate was
approximately 2.8 MBps, a small fraction of the speed at which that can read
from and write to its disks. This performance gap is even more apparent when
one considers that a significant fraction of the data (approximately 27 TB)
could conceivably have been buffered in the cluster's main memory.

These efficiency problems are not limited to Hadoop. Anderson and
Tucek~\cite{efficiency-matters} examined a collection of large-scale
data-intensive processing systems and found a widespread lack of efficiency
among them.

The tempting solution to the problem of inefficiency is to simply increase the
size of the cluster, splitting the data being processed among progressively
more nodes. This decreases both the amount of data that each node must process
and (to the extent allowed by Amdahl's Law~\cite{amdahls_law}) increases the
throughput of the system. However, this approach has several negative
consequences.

Larger clusters have a proportionately large capital expense and operational
cost. Google, one of the pioneers in large-scale data-intensive systems, has
contracted over 260MW to power its data
centers~\cite{google-dc-power-blog}. When it filed its IPO in 2011, Facebook
reported that it spent \$606 million on constructing and equipping its data
centers in 2011 and expected to spend another \$500 million in
2012~\cite{facebook-ipo}. As problem sizes increase, these expenses must by
necessity also increase unless system efficiency is improved.

Large data centers also have an environmental cost. McKinsey and Company
estimates that the carbon dioxide emissions from data centers will surpass
emissions from the airline industry by 2020~\cite{mckinsey-co2-study}.
Further, larger clusters are harder to manage and experience faults more
frequently than smaller clusters do because of the increased number of nodes in
those clusters. We will explore the implications of increased failure further
in Chapter~\ref{chapter:fault_tolerance}.

\section{Sources of Inefficiency in Existing Systems}

While a thorough study of the sources of per-node inefficiency in existing
systems has not been performed, we can broadly classify three different sources
of inefficiency in systems that are I/O-bound:

\paragraph{Inefficient I/O} Current-generation large scale data processing
systems read from and write to large collections of magnetic hard drives. These
magnetic drives are characterized by their fast sequential access and slow
random access. Fundamentally, systems that desire a high throughput from these
devices should write to them sequentially as much as possible. However,
existing systems often treat disks as a ``black box'' without consideration for
the overheads of non-sequential access.

\paragraph{Too much I/O per record} Existing
systems may read and write each record to disk several times during the course
of a job, either because of memory pressure or for increased fault
tolerance. These additional reads and writes incur significant additional
overhead, as writing to disk is at least an order of magnitude slower than
accessing other levels in the node's memory hierarchy.

\paragraph{Imbalanced hardware configurations} Often, the hardware platforms on
which these systems are deployed are configured such that the system will run
out of network bandwidth or memory before they can maximize their disks'
throughput. In Chapter~\ref{chapter:tritonsort}, we argue that a degree of
software/hardware co-design can lead to radically more efficient software and
hardware architectures.

\section{Hypothesis}

The hypothesis of this dissertation is that systems built with efficient disk
I/O as a first-order architectural concern can realize an order of magnitude
improvement in performance versus existing large-scale data-intensive systems
without compromising their scalability or generality.

We argue that the chief challenges of building such a system lie both in
minimizing the number of I/O operations per record and in ensuring that disk
I/O is done sequentially as much as possible. We also argue that significant
increases in per-node efficiency can be realized by considering fault tolerance
models that prioritize efficient I/O both in failure-free operation and during
recovery.

\begin{table}[ht]
\caption{\label{table:system-efficiency} Large scale sorting results over time,
  and their associated per-node and per-disk efficiency. Results extracted from
  ~\cite{efficiency-matters, hadoop-sort-2009, themis, tritonsort}.}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\textbf{Year} & \textbf{Name} & \textbf{Nodes} & \textbf{Disks} & \textbf{MB/s} & \textbf{MB/s/node} & \textbf{MB/s/disk}\\
\hline
2012 & Themis (35TB) & 20 & 320 & 4656 & 232.8 & 14.6 \\
2011 & Themis & 52 & 832 & 12080 & 232.4 & 14.5 \\
2011 & TritonSort & 52 & 832  & 15633 & 300.6 & 18.8 \\
2009 & Hadoop & 3452 & 13808 & 9633 & 2.79 & 0.69 \\
2009 & DEMSort & 195 & 780 & 9400 & 48.2 & 12.1\\
\hline
\end{tabular}
\end{table}

We explore the design of radically more efficient data processing systems
through two main prototype systems: TritonSort, a large-scale sorting system,
and Themis, an implementation of the MapReduce programming
model. Table~\ref{table:system-efficiency} compares the performance of
TritonSort and Themis with previous sort benchmark record holders. TritonSort
and Themis have each improved on the per-node performance of systems in their
respective problem domains by almost an order of magnitude, approaching the
maximum throughput possible on the clusters on which they are deployed. At time
of writing, TritonSort and Themis hold four world records in large-scale
sorting.

\section{Organization}

Chapter~\ref{chapter:background} provides background on the problem domains of
large-scale sorting and MapReduce. Chapter~\ref{chapter:principles} provides an
overview of the architecture and design principles that underpin both the
systems presented in this dissertation. Chapter~\ref{chapter:tritonsort}
presents the design and implementation of TritonSort, our large-scale sorting
system. Chapter~\ref{chapter:themis} presents the design and implementation of
Themis, our MapReduce implementation, focusing in particular on its differences
from TritonSort's design. Chapter~\ref{chapter:fault_tolerance} takes an
in-depth look at fault tolerance in Themis, looking first at the trade-off
between fault tolerance and I/O-efficiency, and then presenting the design and
implementation of an I/O-efficient fault tolerance scheme for
Themis. Chapter~\ref{chapter:related} explores related work.  The dissertation
concludes with Chapter~\ref{chapter:conclusions}, which describes some open
problems and future directions.
