\chapter{Introduction}

The quantity of data the world generates and stores is growing at a staggering
pace. Online retailers like Amazon.com log users' purchase histories and
interactions with their websites in order to target advertising to their
particular interests. Walmart handles more than a million customer transactions
per hour, and the size of its customer database is estimated at 2.5
petabytes~\cite{economist-data-data-everywhere}. Search engines like Google
construct complex indices over the entire public Internet, which is estimated
to consist of at least 6.9 billion pages~\cite{worldwidewebsize}. Facebook's
users upload more than 300 million photos per
day~\cite{jay-parikh-slideshow}. Scientific instruments like the Australian
Square Kilometer Array, the Large Hadron Collider and the Pan-STARRS array of
telescopes can generate petabytes of data per day~\cite{fourth-paradigm}.

Capturing this data, while a technically challenging feat in and of itself, is
not enough. To be useful, this large volume of data must be analyzed,
aggregated, filtered, and explored. This is no easy task. The aforementioned
data sets are but a few examples of a new class of ``big data'' -- data sets
that are so large and complex that they become difficult to process using
existing techniques and technologies.

In recent years, a range of large-scale, data-intensive systems have been
developed to attempt to tackle the analysis of ``big data'' workloads. These
systems typically approach a given data analysis task by breaking the task into
logically separable pieces and then distributing those pieces across a cluster
of tens to tens of thousands of computers.
