\section{Skew Mitigation}
\label{sec:phase_zero}

To satisfy the 2-IO property, Themis must ensure that every partition can be
sorted in memory, since an out-of-core sort would induce additional I/Os.  In
addition, to support parallelism, partitions must be small enough that several
partitions can be processed in parallel.  Phase zero is responsible for
choosing the number of partitions, and selecting a partitioning function to
keep each partition roughly the same size.  This task is complicated by the
fact that the data to be partitioned is generated by the \map function.  Thus,
even if the distribution of input data is known, the distribution of
intermediate data may not be known.  This phase is optional: if the user has
knowledge of the intermediate data's distribution, they can specify a custom
partitioning function, similar to techniques used in Hadoop.

Phase zero approximates the distribution of intermediate data by applying the
\map function to a subset of the input.  If the data is homoscedastic, then a
small prefix of the input is sufficient to approximate the intermediate
distribution.  Otherwise, more input data will need to be sampled, or phase
two's performance will decrease.  DeWitt et al.~\cite{ProbabilisticSplitting}
formalize the number of samples needed to achieve a given skew with high
probability; typically we sample 1 GB per node of input data for nodes
supporting 8 TB of input. The correctness of phase two only depends on
partitions being smaller than main memory.  Since our target partition size is
less than 5\% of main memory, this means that a substantial sampling error
would have to occur to cause job failure.  So although sampling does impose
additional I/O over the 2-IO limit, we note that it is a small and constant
overhead.

Once each node is done sampling, it transmits its sample information to a
central coordinator.  The coordinator uses these samples to generate a
partition function, which is then re-distributed back to each node.

\subsection{Mechanism}

On each node, Themis applies the \map operation to a prefix of the records in
each input file stored on that node.  As the \map function produces records,
the node records information about the intermediate data, such as how much
larger or smaller it is than the input and the number of records generated.  It
also stores information about each intermediate key and the associated record's
size.  This information varies based on the sampling policy.  Once the node is
done sampling, it sends that metadata to the coordinator.

The coordinator merges the metadata from each of the nodes to estimate the
intermediate data size.  It then uses this size, and the desired partition
size, to compute the number of partitions.  Then, it performs a streaming
merge-sort on the samples from each node.  Once all the sampled data is sorted,
partition boundaries are calculated based on the desired partition sizes.  The
result is a list of ``boundary keys'' that define the edges of each partition.
This list is broadcast back to each node, and forms the basis of the
partitioning function used in phase one.

The choice of sampling policy depends on requirements from the user,
and we now describe each policy.

\subsection{Sampling Policies}

Themis supports the following sampling policies:

\paragraph{(1) Range partitioning:}
For MapReduce jobs in which the ultimate output of all the reducers must be
totally ordered (e.g., sort), Themis employs a range partitioning sampling
policy.  In this policy, the entire key for each sampled record is
sent to the coordinator.  A downside of this policy is that very large
keys can limit the amount of data that can be sampled because there is
only a limited amount of space to buffer sampled records.

\paragraph{(2) Hash partitioning:} For situations in which total ordering of
\reduce function output is not required, Themis employs hash partitioning.  In
this scheme, a hash of the key is sampled, instead of the keys themselves.
This has the advantage of supporting very large keys, and allowing Themis to
use reservoir sampling~\cite{Vitter:1985:RSR:3147.3165}, which samples data in
constant space in one pass over its input.  This enables more data to be
sampled with a fixed amount of buffer.  This approach also works well for input
data that is already partially or completely sorted because adjacent keys are
likely to be placed in different partitions, which spreads the data across the
cluster.
