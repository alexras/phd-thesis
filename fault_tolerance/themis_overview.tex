\subsection{Themis: I/O-Efficient MapReduce}
\label{sec:themis}

In this section, we present a brief recap of the design of \themis, our highly
I/O-efficient MapReduce system. A more detailed description and evaluation of
\themis is presented in Chapter~\ref{chapter:themis}.  We opted to implement
our fault tolerance scheme in \themis rather than Hadoop because \themis lacked
a proportional fault tolerance mechanism prior to this work, whereas the
task-level fault tolerance scheme used by Hadoop is a tightly-integrated part
of its design.

\begin{figure}
  \centering
  \begin{subfigure}[t]{\columnwidth}
  \centering
  \includegraphics[width=\columnwidth]{fault_tolerance/figures/detailed_phase_one.pdf}
  \caption{\label{fig:phase_one} Phase One}
  \end{subfigure}\vspace{1em}
  \begin{subfigure}[t]{\columnwidth}
  \centering
  \includegraphics[width=\columnwidth]{fault_tolerance/figures/phase_two.pdf}
  \caption{\label{fig:phase_two} Phase Two}
  \end{subfigure}

  \caption{\label{fig:themis_phases} A diagrammatic overview of \themis' phases.}
\end{figure}

Nodes in a \themis cluster each have a collection of \emph{intermediate disks}
that store volatile intermediate data and a disjoint collection of \emph{DFS
  disks} that store input and output data, and are typically under the control
of a distributed file system like HDFS.

\themis runs a MapReduce job in two main \emph{phases}, called \emph{phase one}
and \emph{phase two}.  In phase one, input records are read in parallel from
the cluster's DFS disks. \themis applies the \map function to each record,
producing a collection of \emph{intermediate records} that are written to
intermediate partitions spread across the cluster's intermediate disks. Each
intermediate partition holds all records with a certain set of keys. The
mapping from keys to intermediate partitions is determined by a \emph{partition
  function}. Phase one is roughly analogous to Hadoop's map and shuffle phases.

At the end of phase one, all intermediate records have been generated,
partitioned and stored across the cluster's intermediate disks. A diagrammatic
overview of phase one is given in Figure~\ref{fig:phase_one}.

In phase two, each intermediate partition is read from the cluster's
intermediate disks completely into memory. Once in memory, it is sorted in-core
by key, and the \reduce function is applied to each group of records in the
partition with the same key. This produces a collection of \emph{output
  records} that are written to files on the DFS disks. Phase two is roughly
equivalent to Hadoop's sort and reduce phases. A diagrammatic overview of phase
two is given in Figure~\ref{fig:phase_two}.

Note that phase one requires all-to-all communication among cluster nodes, but
that phase two can be executed on each node independently.

\subsubsection{Partitioning}

In order for phase two to be processed efficiently, partitions should be small
enough for several of them to be processed in memory
simultaneously. Additionally, they should be as uniformly-sized as possible to
prevent stragglers. The partition function is responsible for ensuring both
these properties. The user can provide their own partition function, or it can
be derived at runtime through an optional sampling phase called \emph{phase
  zero}. Phase zero requires a fairly small sample to produce a good partition
function, and typically takes under a minute to run.
